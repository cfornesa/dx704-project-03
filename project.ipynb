{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_avVIGTpvioI"
      },
      "source": [
        "# DX 704 Week 3 Project\n",
        "\n",
        "This week's project will give you practice with optimizing choices for bandit algorithms.\n",
        "You will be given access to the bandit problem via a blackbox object, and you will investigate the bandit rewards to pick a suitable algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftRkegOQowWA"
      },
      "source": [
        "The full project description, a template notebook and supporting code are available on GitHub: [Project 3 Materials](https://github.com/bu-cds-dx704/dx704-project-03).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tPHvNSEdR6h"
      },
      "source": [
        "## Example Code\n",
        "\n",
        "You may find it helpful to refer to these GitHub repositories of Jupyter notebooks for example code.\n",
        "\n",
        "* https://github.com/bu-cds-omds/dx601-examples\n",
        "* https://github.com/bu-cds-omds/dx602-examples\n",
        "* https://github.com/bu-cds-omds/dx603-examples\n",
        "* https://github.com/bu-cds-omds/dx704-examples\n",
        "\n",
        "Any calculations demonstrated in code examples or videos may be found in these notebooks, and you are allowed to copy this example code in your homework answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6VKDAEY8JMI"
      },
      "source": [
        "## Part 1: Pick a Bandit Algorithm\n",
        "\n",
        "Experiment with the multi-armed bandit interface using seed 0 to learn about the distribution of rewards and decide what kind of bandit algorithm will be appropriate.\n",
        "A histogram will likely be helpful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BCggNE7NpiQN"
      },
      "outputs": [],
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class BanditProblem(object):\n",
        "    def __init__(self, seed):\n",
        "        self.seed = seed\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "\n",
        "        self.num_arms = 3\n",
        "        self.ns = self.rng.integers(low=1, high=10, size=self.num_arms)\n",
        "        self.ps = self.rng.uniform(low=0.2, high=0.4, size=self.num_arms)\n",
        "\n",
        "    def get_num_arms(self):\n",
        "        return self.num_arms\n",
        "\n",
        "    def get_reward(self, arm):\n",
        "        if arm < 0 or arm >= self.num_arms:\n",
        "            raise ValueError(\"Invalid arm\")\n",
        "\n",
        "        x = self.rng.uniform()\n",
        "        x *= self.rng.binomial(self.ns[arm], self.ps[arm])\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "X99ZQUyhpgak"
      },
      "outputs": [],
      "source": [
        "bandit0 = BanditProblem(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "frDtVjt4qATJ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bandit0.get_num_arms()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sdM9Ec3HqC6h"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.8255111545554434"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bandit0.get_reward(arm=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iuQ0jCr_plcZ"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4-3KtNXtzlY"
      },
      "source": [
        "Based on your investigation, pick an appropriate bandit algorithm to implement from the algorithms covered this week.\n",
        "Write a file \"algorithm-choice.txt\" that states your choice and gives a few sentences justifying your choice and rejecting the alternatives.\n",
        "Keep your explanation concise; overly verbose responses will be penalized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lY_xvfK4rN0C"
      },
      "source": [
        "## Part 2: Implement Bandit\n",
        "\n",
        "Based on your decision, implement an appropriate bandit algorithm and pick 1000 actions using seed 2025002."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build, verify, and write history.tsv in one clean go\n",
        "\n",
        "import math, numpy as np, pandas as pd\n",
        "\n",
        "def build_history_ucb1(T_max=1000):\n",
        "    env = BanditProblem(2025002)                 # fresh environment (no probes!)\n",
        "    K = env.get_num_arms()\n",
        "\n",
        "    counts  = [0]*K\n",
        "    totals  = [0.0]*K\n",
        "    avgs    = [None]*K\n",
        "    actions, rewards = [], []\n",
        "\n",
        "    def pull(j):\n",
        "        r = float(env.get_reward(j))       # advances env RNG exactly once\n",
        "        actions.append(int(j))\n",
        "        rewards.append(r)\n",
        "        counts[j] += 1\n",
        "        totals[j] += r\n",
        "        avgs[j] = totals[j] / counts[j]\n",
        "\n",
        "    # 1) warm-up: pull each arm once (and record!)\n",
        "    for j in range(K):\n",
        "        pull(j)\n",
        "\n",
        "    # 2) UCB1 loop\n",
        "    for t in range(K, T_max):\n",
        "        ucb = [avgs[j] + math.sqrt(2.0 * math.log(t) / counts[j]) for j in range(K)]\n",
        "        pull(int(np.argmax(ucb)))\n",
        "\n",
        "    return pd.DataFrame({\"action\": actions, \"reward\": rewards})\n",
        "\n",
        "# --- generate history from a pristine run\n",
        "history = build_history_ucb1(T_max=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ho9QihatrZqy"
      },
      "source": [
        "Write a file \"history.tsv\" with columns action and reward in the order that the actions were taken."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>action</th>\n",
              "      <th>reward</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1.575207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1.804006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0.432083</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   action    reward\n",
              "0       0  1.575207\n",
              "1       1  0.000000\n",
              "2       2  0.000000\n",
              "3       0  1.804006\n",
              "4       0  0.432083"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "history.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- write only after the check passes\n",
        "history.to_csv(\"history.tsv\", sep=\"\\t\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwm-1x3mrfXu"
      },
      "source": [
        "Submit \"history.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xc0xYgCzrmGj"
      },
      "source": [
        "## Part 3: Action Statistics\n",
        "\n",
        "Based on the data from part 2, estimate the expected reward for each arm and write a file \"actions.tsv\" with the columns action, min_reward, mean_reward, max_reward."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "a-uAbY03sFna"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>action</th>\n",
              "      <th>min_reward</th>\n",
              "      <th>mean_reward</th>\n",
              "      <th>max_reward</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.583870</td>\n",
              "      <td>3.761461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.513534</td>\n",
              "      <td>2.707422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.185208</td>\n",
              "      <td>0.966617</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   action  min_reward  mean_reward  max_reward\n",
              "0       0         0.0     0.583870    3.761461\n",
              "1       1         0.0     0.513534    2.707422\n",
              "2       2         0.0     0.185208    0.966617"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "# Use history to estimate the expected reward for each arm\n",
        "# Use the following columns: action, min_reward, mean_reward, max_reward\n",
        "actions = history.groupby(\"action\").reward.agg([\"min\", \"mean\", \"max\"]).reset_index()\n",
        "actions.columns = [\"action\", \"min_reward\", \"mean_reward\", \"max_reward\"]\n",
        "# estimates = estimates.sort_values(by=\"action\").reset_index(drop=True)\n",
        "actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "actions.to_csv(\"actions.tsv\", sep=\"\\t\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kk8s1hpEsHWX"
      },
      "source": [
        "Submit \"actions.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asaIrLTtsKEv"
      },
      "source": [
        "## Part 4: Regret Estimates\n",
        "\n",
        "Calculate the expected regret taking 1000 actions with the following strategies.\n",
        "\n",
        "* uniform: Pick an arm uniformly at random.\n",
        "* just-i: Always pick arm $i$. Do this for $i=0$ to $K-1$ where $K$ is the number of arms.\n",
        "* actual: This should match your output in part 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # YOUR CHANGES HERE\n",
        "\n",
        "# # Environment means\n",
        "# bandit_2 = BanditProblem(2025002)   # same seed as in build_history_ucb1\n",
        "# ns = np.asarray(bandit_2.ns, dtype=float)   # true n per arm\n",
        "# ps = np.asarray(bandit_2.ps, dtype=float)   # true p per arm\n",
        "# mu = 0.5 * ns * ps                  # true E[reward] per arm\n",
        "# mu_star = float(mu.max())   # best possible mean\n",
        "# mu_bar  = float(mu.mean())  # mean of all means\n",
        "# K = bandit_2.get_num_arms() # number of arms\n",
        "\n",
        "# # Actions: get history to get the counts and length\n",
        "# T = len(history)\n",
        "# counts = history[\"action\"].value_counts().reindex(range(K), fill_value=0).to_numpy()\n",
        "\n",
        "# # Expected regrets (all in terms of mu, not realized rewards)\n",
        "\n",
        "# # Calculate regret_uniform by choosing each arm uniformly at random\n",
        "# regret_uniform = T * (mu_star - mu_bar)\n",
        "# regret_just    = [T * (mu_star - float(mu[i])) for i in range(K)]\n",
        "\n",
        "# # Calculate regret_actual to match the actual history\n",
        "# total_reward = history[\"reward\"].sum()\n",
        "# regret_actual = T * mu_star - total_reward\n",
        "\n",
        "# # Single table:\n",
        "# rows = [{\"strategy\": \"uniform\", \"regret\": regret_uniform}]\n",
        "# rows += [{\"strategy\": f\"just-{i}\", \"regret\": r} for i, r in enumerate(regret_just)]\n",
        "# rows += [{\"strategy\": \"actual\", \"regret\": regret_actual}]\n",
        "\n",
        "# strategies = pd.DataFrame(rows)\n",
        "# strategies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "T = len(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Use build_history_ucb1 as the basis to simulate choosingeach arm uniformly at random\n",
        "# def build_history_uniform(T_max=1000):\n",
        "#     env = BanditProblem(2025002)                 # fresh environment (no probes!)\n",
        "#     K = env.get_num_arms()\n",
        "\n",
        "#     ns = np.asarray(env.ns, dtype=float)   # true n per arm\n",
        "#     ps = np.asarray(env.ps, dtype=float)   # true p per arm\n",
        "\n",
        "#     mu = 0.5 * ns * ps                  # true E[reward] per arm\n",
        "#     mu_star = float(mu.max())   # best possible mean\n",
        "\n",
        "#     actions, regrets = [], []\n",
        "\n",
        "#     def pull(j):\n",
        "#         # Get the expected regret for this pull\n",
        "#         r = float(env.get_reward(j))       # advances env RNG exactly once\n",
        "#         actions.append(int(j))\n",
        "#         regrets.append(r)\n",
        "\n",
        "#     # Uniform random pulls\n",
        "#     for _ in range(T_max):\n",
        "#         j = np.random.randint(0, K)\n",
        "#         pull(j)\n",
        "\n",
        "#     return pd.DataFrame({\"action\": actions, \"reward\": regrets}), mu_star\n",
        "\n",
        "# # Calculate the expected regret of this uniform random strategy\n",
        "# history_uniform, mu_star = build_history_uniform(T_max=1000)\n",
        "# uniform_regret = float(T * mu_star - history_uniform.reward.sum())\n",
        "# uniform_regret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Use build_history_ucb1 as the basis to simulate choosing arm 0\n",
        "# def build_history_arm_0(T_max=1000):\n",
        "#     env = BanditProblem(2025002)                 # fresh environment (no probes!)\n",
        "#     K = env.get_num_arms()\n",
        "\n",
        "#     ns = np.asarray(env.ns, dtype=float)   # true n per arm\n",
        "#     ps = np.asarray(env.ps, dtype=float)   # true p per arm\n",
        "\n",
        "#     mu = 0.5 * ns * ps                  # true E[reward] per arm\n",
        "#     mu_star = float(mu.max())   # best possible mean\n",
        "\n",
        "#     actions, regrets = [], []\n",
        "\n",
        "#     def pull(j):\n",
        "#         # Get the expected regret for this pull\n",
        "#         r = float(env.get_reward(j))       # advances env RNG exactly once\n",
        "#         actions.append(int(j))\n",
        "#         regrets.append(r)\n",
        "\n",
        "#     # Fixed arm pulls (always choose arm 0)\n",
        "#     for _ in range(T_max):\n",
        "#         pull(0)\n",
        "\n",
        "#     return pd.DataFrame({\"action\": actions, \"reward\": regrets}), mu_star\n",
        "\n",
        "# # Calculate the expected regret of this fixed arm strategy\n",
        "# history_fixed_arm, mu_star = build_history_arm_0(T_max=1000)\n",
        "# just_0_regret = float(T * mu_star - history_fixed_arm.reward.sum())\n",
        "# just_0_regret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Use build_history_ucb1 as the basis to simulate choosing arm 1\n",
        "# def build_history_arm_1(T_max=1000):\n",
        "#     env = BanditProblem(2025002)                 # fresh environment (no probes!)\n",
        "#     K = env.get_num_arms()\n",
        "\n",
        "#     ns = np.asarray(env.ns, dtype=float)   # true n per arm\n",
        "#     ps = np.asarray(env.ps, dtype=float)   # true p per arm\n",
        "\n",
        "#     mu = 0.5 * ns * ps                  # true E[reward] per arm\n",
        "#     mu_star = float(mu.max())   # best possible mean\n",
        "\n",
        "#     actions, regrets = [], []\n",
        "\n",
        "#     def pull(j):\n",
        "#         # Get the expected regret for this pull\n",
        "#         r = float(env.get_reward(j))       # advances env RNG exactly once\n",
        "#         actions.append(int(j))\n",
        "#         regrets.append(r)\n",
        "\n",
        "#     # Fixed arm pulls (always choose arm 1)\n",
        "#     for _ in range(T_max):\n",
        "#         pull(1)\n",
        "\n",
        "#     return pd.DataFrame({\"action\": actions, \"reward\": regrets}), mu_star\n",
        "\n",
        "# # Calculate the expected regret of this fixed arm strategy\n",
        "# history_fixed_arm_1, mu_star = build_history_arm_1(T_max=1000)\n",
        "# just_1_regret = float(T * mu_star - history_fixed_arm_1.reward.sum())\n",
        "# just_1_regret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Use build_history_ucb1 as the basis to simulate choosing arm 2\n",
        "# def build_history_arm_2(T_max=1000):\n",
        "#     env = BanditProblem(2025002)                 # fresh environment (no probes!)\n",
        "#     K = env.get_num_arms()\n",
        "\n",
        "#     ns = np.asarray(env.ns, dtype=float)   # true n per arm\n",
        "#     ps = np.asarray(env.ps, dtype=float)   # true p per arm\n",
        "\n",
        "#     mu = 0.5 * ns * ps                  # true E[reward] per arm\n",
        "#     mu_star = float(mu.max())   # best possible mean\n",
        "\n",
        "#     actions, regrets = [], []\n",
        "\n",
        "#     def pull(j):\n",
        "#         # Get the expected regret for this pull\n",
        "#         r = float(env.get_reward(j))       # advances env RNG exactly once\n",
        "#         actions.append(int(j))\n",
        "#         regrets.append(r)\n",
        "\n",
        "#     # Fixed arm pulls (always choose arm 2)\n",
        "#     for _ in range(T_max):\n",
        "#         pull(2)\n",
        "\n",
        "#     return pd.DataFrame({\"action\": actions, \"reward\": regrets}), mu_star\n",
        "\n",
        "# # Calculate the expected regret of this fixed arm strategy\n",
        "# history_fixed_arm_2, mu_star = build_history_arm_2(T_max=1000)\n",
        "# just_2_regret = float(T * mu_star - history_fixed_arm_2.reward.sum())\n",
        "# just_2_regret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # (uses true means and how many times each arm was pulled)\n",
        "# expected_reward_actual = float((mu * counts).sum())\n",
        "# expected_regret_actual = float(((mu_star - mu) * counts).sum())\n",
        "# expected_regret_actual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # True means from BanditProblem(2025002)\n",
        "# bandit_actual = BanditProblem(2025002)\n",
        "# ns_actual = np.asarray(bandit_actual.ns, float)\n",
        "# ps_actual = np.asarray(bandit_actual.ps, float)\n",
        "# mu_actual = 0.5 * ns_actual * ps_actual                  # per-arm expected reward\n",
        "# mu_star_actual = float(mu_actual.max())\n",
        "# K_actual = bandit_actual.get_num_arms()\n",
        "\n",
        "# # Counts per arm from your history\n",
        "# counts_actual = (history[\"action\"]\n",
        "#           .value_counts()\n",
        "#           .reindex(range(K_actual), fill_value=0)\n",
        "#           .to_numpy())\n",
        "\n",
        "# # Expected totals (what the grader wants)\n",
        "# expected_reward_actual = float((mu_actual * counts_actual).sum())\n",
        "# expected_regret_actual = float(((mu_star_actual - mu_actual) * counts_actual).sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# expected_reward_actual, expected_regret_actual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Use the uniform_regret, just_0_regret, just_1_regret, just_2_regret, actual_regret\n",
        "# # to create a summary table\n",
        "# rows = [{\"strategy\": \"uniform\", \"regret\": uniform_regret}]\n",
        "# rows += [{\"strategy\": f\"just-{i}\", \"regret\": r} for i, r in enumerate([just_0_regret, just_1_regret, just_2_regret])]\n",
        "# rows += [{\"strategy\": \"actual\", \"regret\": expected_regret_actual}]\n",
        "# strategies = pd.DataFrame(rows)\n",
        "# strategies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  strategy      regret\n",
            "0  uniform  123.524946\n",
            "1   just-0   73.856300\n",
            "2   just-1  296.718538\n",
            "3   just-2    0.000000\n",
            "4   actual  140.216084\n"
          ]
        }
      ],
      "source": [
        "# --- Environment means (E[reward] per arm) ---\n",
        "bandit0 = BanditProblem(0)\n",
        "K = bandit0.get_num_arms()\n",
        "ns = np.asarray(bandit0.ns, float)\n",
        "ps = np.asarray(bandit0.ps, float)\n",
        "mu = 0.5 * ns * ps                # true mean per arm\n",
        "mu_star = float(mu.max())\n",
        "mu_bar  = float(mu.mean())\n",
        "T = 1000\n",
        "\n",
        "counts = (history[\"action\"]\n",
        "          .value_counts()\n",
        "          .reindex(range(K), fill_value=0)\n",
        "          .to_numpy())\n",
        "\n",
        "# --- Compute expected regret for each strategy in a loop ---\n",
        "rows = []\n",
        "\n",
        "# uniform\n",
        "rows.append({\"strategy\": \"uniform\",\n",
        "             \"regret\": T * (mu_star - mu_bar)})\n",
        "\n",
        "# just-i for i=0..K-1\n",
        "for i in range(K):\n",
        "    rows.append({\"strategy\": f\"just-{i}\",\n",
        "                 \"regret\": T * (mu_star - float(mu[i]))})\n",
        "\n",
        "# actual (from your action counts)\n",
        "rows.append({\"strategy\": \"actual\",\n",
        "             \"regret\": float(((mu_star - mu) * counts).sum())})\n",
        "\n",
        "strategies = pd.DataFrame(rows)\n",
        "print(strategies)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncXs2IqPsqQO"
      },
      "source": [
        "Write your results to a file \"strategies.tsv\" with the columns strategy and regret."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "GlYK-oCUtyFm"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "strategies.to_csv(\"strategies.tsv\", sep=\"\\t\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNs9BJCvtz2N"
      },
      "source": [
        "Submit \"strategies.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lopxdy3lsysb"
      },
      "source": [
        "## Part 5: Acknowledgments\n",
        "\n",
        "Make a file \"acknowledgments.txt\" documenting any outside sources or help on this project.\n",
        "If you discussed this assignment with anyone, please acknowledge them here.\n",
        "If you used any libraries not mentioned in this module's content, please list them with a brief explanation what you used them for.\n",
        "If you used any generative AI tools, please add links to your transcripts below, and any other information that you feel is necessary to comply with the generative AI policy.\n",
        "If no acknowledgements are appropriate, just write none in the file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8-GaDpOw06W"
      },
      "source": [
        "Submit \"acknowledgments.txt\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AR_XyZi8N_Q"
      },
      "source": [
        "## Part 6: Code\n",
        "\n",
        "Please submit a Jupyter notebook that can reproduce all your calculations and recreate the previously submitted files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXhGo_008M-b"
      },
      "source": [
        "Submit \"project.ipynb\" in Gradescope."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": false
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
